import tempfile

import streamlit as st

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.schema import Document

from googleapiclient.discovery import build

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Streamlit Secretsì—ì„œ API í‚¤ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY")
GOOGLE_API_KEY = st.secrets.get("GOOGLE_API_KEY")
GOOGLE_CSE_ID = st.secrets.get("GOOGLE_CSE_ID")

if not OPENAI_API_KEY:
    st.error("Streamlit Secretsì— OPENAI_API_KEYë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”.")
if not GOOGLE_API_KEY or not GOOGLE_CSE_ID:
    st.error("Streamlit Secretsì— GOOGLE_API_KEYì™€ GOOGLE_CSE_IDë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ PDF â†’ Document ë¦¬ìŠ¤íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_and_split_pdf(file_path: str):
    loader = PyPDFLoader(file_path)
    pages = loader.load_and_split()
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    return splitter.split_documents(pages)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì›¹ê²€ìƒ‰ â†’ Document ë¦¬ìŠ¤íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def web_search_docs(query: str, num_results: int = 5):
    service = build("customsearch", "v1", developerKey=GOOGLE_API_KEY)
    res = service.cse().list(q=query, cx=GOOGLE_CSE_ID, num=num_results).execute()
    items = res.get("items", [])
    docs = []
    for item in items:
        title = item.get("title", "")
        snippet = item.get("snippet", "")
        link = item.get("link", "")
        content = f"{title}\n{snippet}"
        docs.append(Document(page_content=content, metadata={"source": link}))
    return docs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ QA ì²´ì¸ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€
def setup_qa_chain(documents, model_name: str = "gpt-4"):
    vector_store = FAISS.from_documents(documents, OpenAIEmbeddings())
    llm = ChatOpenAI(model=model_name, openai_api_key=OPENAI_API_KEY)
    retriever = vector_store.as_retriever()
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        return_source_documents=True
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ rerun í˜¸í™˜ì„± ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rerun():
    if hasattr(st, "rerun"):
        st.rerun()
    else:
        st.experimental_rerun()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Streamlit ì•± â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_app():
    st.set_page_config(page_title="íšŒì›ê°€ì… ê³ ê°ìƒë‹´ ì±—ë´‡", page_icon="ğŸ¤–", layout="wide")

    st.markdown(
        """
        <h1 style="text-align:center; color:#6C63FF;">ğŸ“š íšŒì›ê°€ì… ê³ ê°ìƒë‹´ ì±—ë´‡</h1>
        <p style="text-align:center;">ì—…ë¡œë“œí•œ <strong>PDF FAQ</strong>ì™€ <strong>ì›¹ê²€ìƒ‰</strong>ì„ í†µí•´ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.</p>
        """,
        unsafe_allow_html=True
    )

    st.sidebar.header("âš™ï¸ ì„¤ì •")
    st.sidebar.markdown("FAQ PDF ì—…ë¡œë“œ ë° ì›¹ê²€ìƒ‰ ì§ˆë¬¸ ê¸°ëŠ¥ ì œê³µ")

    if "history" not in st.session_state:
        st.session_state.history = []

    if "uploaded_file" not in st.session_state:
        st.session_state.uploaded_file = None

    # PDF ì—…ë¡œë“œ
    st.sidebar.subheader("ğŸ“„ FAQ PDF ì—…ë¡œë“œ")
    uploaded = st.sidebar.file_uploader("PDF ì—…ë¡œë“œ", type=["pdf"])
    if uploaded:
        st.session_state.uploaded_file = uploaded
        st.sidebar.success("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ!")

    # ì§ˆë¬¸ ì…ë ¥
    st.markdown("### â“ ì§ˆë¬¸ ì…ë ¥")
    query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", value="", key="query_input")

    if st.button("ğŸ¤” ì§ˆë¬¸í•˜ê¸°"):
        actual_query = query.strip()
        pdf_answer = None
        pdf_sources = []

        if actual_query:
            if st.session_state.uploaded_file:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                    tmp.write(st.session_state.uploaded_file.read())
                    tmp_path = tmp.name

                pdf_docs = load_and_split_pdf(tmp_path)
                pdf_chain = setup_qa_chain(pdf_docs)
                pdf_res = pdf_chain.invoke({"query": actual_query})
                pdf_answer = pdf_res.get("result", "").strip()
                pdf_sources = pdf_res.get("source_documents", [])

            st.session_state.history.append({
                "query": actual_query,
                "pdf_answer": pdf_answer,
                "pdf_sources": pdf_sources,
                "web_answer": None,
                "web_sources": [],
                "web_searched": False
            })

            rerun()

    # ì´ì „ ì§ˆë¬¸ ê²°ê³¼ ì¶œë ¥
    if st.session_state.history:
        st.markdown("## ğŸ“œ ì´ì „ ì§ˆë¬¸")
        for i, qa in enumerate(st.session_state.history):
            with st.container():
                st.markdown(f"### â“ ì§ˆë¬¸ {i+1}: `{qa['query']}`")

                with st.expander("ğŸ“„ PDF ê¸°ë°˜ ë‹µë³€ ë³´ê¸°"):
                    if qa["pdf_answer"]:
                        st.success(qa["pdf_answer"])
                    else:
                        st.info("PDF ê¸°ë°˜ ë‹µë³€ ì—†ìŒ.")

                if qa["web_searched"]:
                    with st.expander("ğŸŒ ì›¹ê²€ìƒ‰ ê¸°ë°˜ ë‹µë³€ ë³´ê¸°"):
                        if qa["web_answer"]:
                            st.warning(qa["web_answer"])
                        else:
                            st.info("ì›¹ê²€ìƒ‰ ë‹µë³€ ì—†ìŒ.")
                        st.markdown("ğŸ“‘ **ì°¸ê³  ìë£Œ:**")
                        for doc in qa["web_sources"]:
                            url = doc.metadata.get("source", "")
                            st.markdown(f"- ğŸŒ [ì¶œì²˜]({url})")

                if not qa["web_searched"]:
                    if st.button(f"ğŸŒ ì›¹ê²€ìƒ‰ ë‹µë³€ - ì§ˆë¬¸ {i+1}", key=f"web_search_{i}"):
                        web_docs = web_search_docs(qa["query"])
                        web_chain = setup_qa_chain(web_docs)
                        web_res = web_chain.invoke({"query": qa["query"]})
                        qa["web_answer"] = web_res.get("result", "").strip()
                        qa["web_sources"] = web_res.get("source_documents", [])
                        qa["web_searched"] = True
                        rerun()

    # ì—…ë¡œë“œ ì´ˆê¸°í™”
    if st.sidebar.button("ğŸ§¹ ì—…ë¡œë“œ ì´ˆê¸°í™”"):
        st.session_state.uploaded_file = None
        rerun()

    # ë³„ë„ ì›¹ê²€ìƒ‰
    st.markdown("---")
    st.markdown("## ğŸ” ì›¹ê²€ìƒ‰ ì „ìš©")
    web_query = st.text_input("ì›¹ì—ì„œ ê²€ìƒ‰í•  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", key="web_query_input")
    if st.button("ğŸŒ ì›¹ê²€ìƒ‰ë§Œ í•˜ê¸°", key="web_search_direct"):
        if web_query.strip():
            with st.spinner("ì›¹ì—ì„œ ì •ë³´ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤..."):
                web_docs = web_search_docs(web_query.strip())
                web_chain = setup_qa_chain(web_docs)
                web_res = web_chain.invoke({"query": web_query.strip()})

                st.markdown("### ğŸŒ ì›¹ê²€ìƒ‰ ë‹µë³€:")
                st.code(web_res.get("result", "").strip(), language="markdown")

                st.markdown("### ğŸ“‘ ì°¸ê³  ìë£Œ:")
                for doc in web_res.get("source_documents", []):
                    url = doc.metadata.get("source", "")
                    st.markdown(f"- ğŸŒ [ì°¸ê³  ìë£Œ]({url})")

if __name__ == "__main__":
    run_app()
